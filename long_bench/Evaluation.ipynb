{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b363a9ff-05ad-4310-94d6-2a4e091e7bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f1dff04-c9bc-4542-bec2-37777c90c9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = ['Llama-2-7b-chat-hf_1_2_1_2',\n",
    " 'Llama-2-7b-chat-hf_1_2_1_4',\n",
    " 'Llama-2-7b-chat-hf_1_2_2_2',\n",
    " 'Llama-2-7b-chat-hf_1_2_2_4',\n",
    " 'Llama-2-7b-chat-hf_1_4_1_2',\n",
    " 'Llama-2-7b-chat-hf_1_4_1_4',\n",
    " 'Llama-2-7b-chat-hf_1_4_2_2',\n",
    " 'Llama-2-7b-chat-hf_1_4_2_4',\n",
    " 'Llama-2-7b-chat-hf_1_4_3_2',\n",
    " 'Llama-2-7b-chat-hf_1_4_3_4',\n",
    " 'Llama-2-7b-chat-hf_1_4_4_2',\n",
    " 'Llama-2-7b-chat-hf_1_4_4_4',\n",
    " 'Llama-2-7b-chat-hf_3_4_1_2',\n",
    " 'Llama-2-7b-chat-hf_3_4_1_4',\n",
    " 'Llama-2-7b-chat-hf_3_4_2_2',\n",
    " 'Llama-2-7b-chat-hf_3_4_2_4',\n",
    " 'Llama-2-7b-chat-hf_all_2',\n",
    " 'Llama-2-7b-chat-hf_all_4',\n",
    " 'Llama-2-7b-chat-hf_zero_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "10344687-8060-4346-8bd8-a0532581c986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lsht</th>\n",
       "      <th>passage_retrieval_en</th>\n",
       "      <th>passage_retrieval_zh</th>\n",
       "      <th>multifieldqa_en</th>\n",
       "      <th>samsum</th>\n",
       "      <th>multi_news</th>\n",
       "      <th>repobench-p</th>\n",
       "      <th>qmsum</th>\n",
       "      <th>passage_count</th>\n",
       "      <th>vcsum</th>\n",
       "      <th>...</th>\n",
       "      <th>2wikimqa</th>\n",
       "      <th>triviaqa</th>\n",
       "      <th>narrativeqa</th>\n",
       "      <th>trec</th>\n",
       "      <th>multifieldqa_zh</th>\n",
       "      <th>qasper</th>\n",
       "      <th>hotpotqa</th>\n",
       "      <th>gov_report</th>\n",
       "      <th>experiment</th>\n",
       "      <th>avg_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>17.25</td>\n",
       "      <td>8.50</td>\n",
       "      <td>11.00</td>\n",
       "      <td>36.47</td>\n",
       "      <td>41.41</td>\n",
       "      <td>25.51</td>\n",
       "      <td>52.30</td>\n",
       "      <td>21.35</td>\n",
       "      <td>3.69</td>\n",
       "      <td>0.14</td>\n",
       "      <td>...</td>\n",
       "      <td>23.25</td>\n",
       "      <td>83.09</td>\n",
       "      <td>18.97</td>\n",
       "      <td>64.0</td>\n",
       "      <td>8.06</td>\n",
       "      <td>25.47</td>\n",
       "      <td>24.37</td>\n",
       "      <td>26.80</td>\n",
       "      <td>Llama-2-7b-chat-hf_zero_2</td>\n",
       "      <td>26.845238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>17.75</td>\n",
       "      <td>8.50</td>\n",
       "      <td>9.50</td>\n",
       "      <td>36.44</td>\n",
       "      <td>41.36</td>\n",
       "      <td>25.70</td>\n",
       "      <td>52.25</td>\n",
       "      <td>20.93</td>\n",
       "      <td>3.79</td>\n",
       "      <td>0.13</td>\n",
       "      <td>...</td>\n",
       "      <td>22.62</td>\n",
       "      <td>83.59</td>\n",
       "      <td>19.23</td>\n",
       "      <td>64.0</td>\n",
       "      <td>8.08</td>\n",
       "      <td>25.42</td>\n",
       "      <td>24.72</td>\n",
       "      <td>26.35</td>\n",
       "      <td>Llama-2-7b-chat-hf_1_4_4_4</td>\n",
       "      <td>26.795714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17.50</td>\n",
       "      <td>8.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>38.73</td>\n",
       "      <td>41.45</td>\n",
       "      <td>25.81</td>\n",
       "      <td>52.08</td>\n",
       "      <td>20.72</td>\n",
       "      <td>3.18</td>\n",
       "      <td>0.14</td>\n",
       "      <td>...</td>\n",
       "      <td>23.17</td>\n",
       "      <td>83.51</td>\n",
       "      <td>19.14</td>\n",
       "      <td>64.0</td>\n",
       "      <td>8.68</td>\n",
       "      <td>25.14</td>\n",
       "      <td>22.11</td>\n",
       "      <td>26.19</td>\n",
       "      <td>Llama-2-7b-chat-hf_1_2_2_4</td>\n",
       "      <td>26.737619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18.25</td>\n",
       "      <td>8.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>39.75</td>\n",
       "      <td>41.51</td>\n",
       "      <td>25.35</td>\n",
       "      <td>51.76</td>\n",
       "      <td>20.48</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0.14</td>\n",
       "      <td>...</td>\n",
       "      <td>22.78</td>\n",
       "      <td>83.09</td>\n",
       "      <td>19.25</td>\n",
       "      <td>64.0</td>\n",
       "      <td>8.08</td>\n",
       "      <td>23.01</td>\n",
       "      <td>24.91</td>\n",
       "      <td>26.63</td>\n",
       "      <td>Llama-2-7b-chat-hf_all_4</td>\n",
       "      <td>26.720952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>18.25</td>\n",
       "      <td>9.00</td>\n",
       "      <td>8.50</td>\n",
       "      <td>38.43</td>\n",
       "      <td>40.84</td>\n",
       "      <td>25.60</td>\n",
       "      <td>52.56</td>\n",
       "      <td>20.62</td>\n",
       "      <td>3.73</td>\n",
       "      <td>0.14</td>\n",
       "      <td>...</td>\n",
       "      <td>23.56</td>\n",
       "      <td>83.34</td>\n",
       "      <td>18.65</td>\n",
       "      <td>64.0</td>\n",
       "      <td>7.70</td>\n",
       "      <td>22.73</td>\n",
       "      <td>23.94</td>\n",
       "      <td>26.80</td>\n",
       "      <td>Llama-2-7b-chat-hf_1_4_2_4</td>\n",
       "      <td>26.697143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18.25</td>\n",
       "      <td>9.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>38.90</td>\n",
       "      <td>41.25</td>\n",
       "      <td>25.33</td>\n",
       "      <td>51.91</td>\n",
       "      <td>20.69</td>\n",
       "      <td>4.11</td>\n",
       "      <td>0.15</td>\n",
       "      <td>...</td>\n",
       "      <td>22.93</td>\n",
       "      <td>83.17</td>\n",
       "      <td>19.19</td>\n",
       "      <td>64.0</td>\n",
       "      <td>7.60</td>\n",
       "      <td>23.44</td>\n",
       "      <td>23.67</td>\n",
       "      <td>26.43</td>\n",
       "      <td>Llama-2-7b-chat-hf_1_2_1_4</td>\n",
       "      <td>26.664762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>17.75</td>\n",
       "      <td>8.00</td>\n",
       "      <td>9.00</td>\n",
       "      <td>36.62</td>\n",
       "      <td>41.26</td>\n",
       "      <td>25.51</td>\n",
       "      <td>51.80</td>\n",
       "      <td>21.07</td>\n",
       "      <td>3.12</td>\n",
       "      <td>0.15</td>\n",
       "      <td>...</td>\n",
       "      <td>22.26</td>\n",
       "      <td>83.34</td>\n",
       "      <td>19.24</td>\n",
       "      <td>64.0</td>\n",
       "      <td>8.04</td>\n",
       "      <td>25.21</td>\n",
       "      <td>24.55</td>\n",
       "      <td>26.69</td>\n",
       "      <td>Llama-2-7b-chat-hf_1_4_1_4</td>\n",
       "      <td>26.659048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>17.75</td>\n",
       "      <td>8.50</td>\n",
       "      <td>7.50</td>\n",
       "      <td>38.93</td>\n",
       "      <td>41.75</td>\n",
       "      <td>25.06</td>\n",
       "      <td>52.10</td>\n",
       "      <td>20.45</td>\n",
       "      <td>4.06</td>\n",
       "      <td>0.15</td>\n",
       "      <td>...</td>\n",
       "      <td>23.60</td>\n",
       "      <td>82.76</td>\n",
       "      <td>18.83</td>\n",
       "      <td>64.0</td>\n",
       "      <td>7.97</td>\n",
       "      <td>23.18</td>\n",
       "      <td>24.19</td>\n",
       "      <td>26.68</td>\n",
       "      <td>Llama-2-7b-chat-hf_3_4_1_4</td>\n",
       "      <td>26.652381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>17.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>9.00</td>\n",
       "      <td>37.32</td>\n",
       "      <td>41.24</td>\n",
       "      <td>25.72</td>\n",
       "      <td>52.18</td>\n",
       "      <td>20.99</td>\n",
       "      <td>3.14</td>\n",
       "      <td>0.14</td>\n",
       "      <td>...</td>\n",
       "      <td>23.72</td>\n",
       "      <td>83.26</td>\n",
       "      <td>19.20</td>\n",
       "      <td>64.0</td>\n",
       "      <td>7.69</td>\n",
       "      <td>24.51</td>\n",
       "      <td>23.23</td>\n",
       "      <td>26.52</td>\n",
       "      <td>Llama-2-7b-chat-hf_1_4_3_4</td>\n",
       "      <td>26.607143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>18.75</td>\n",
       "      <td>7.00</td>\n",
       "      <td>9.50</td>\n",
       "      <td>39.06</td>\n",
       "      <td>40.95</td>\n",
       "      <td>25.69</td>\n",
       "      <td>51.89</td>\n",
       "      <td>20.51</td>\n",
       "      <td>3.45</td>\n",
       "      <td>0.14</td>\n",
       "      <td>...</td>\n",
       "      <td>23.34</td>\n",
       "      <td>82.88</td>\n",
       "      <td>18.83</td>\n",
       "      <td>64.0</td>\n",
       "      <td>7.68</td>\n",
       "      <td>22.86</td>\n",
       "      <td>23.43</td>\n",
       "      <td>26.43</td>\n",
       "      <td>Llama-2-7b-chat-hf_3_4_2_4</td>\n",
       "      <td>26.558095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>16.75</td>\n",
       "      <td>8.50</td>\n",
       "      <td>10.25</td>\n",
       "      <td>36.75</td>\n",
       "      <td>41.01</td>\n",
       "      <td>25.36</td>\n",
       "      <td>51.52</td>\n",
       "      <td>20.83</td>\n",
       "      <td>3.88</td>\n",
       "      <td>0.17</td>\n",
       "      <td>...</td>\n",
       "      <td>22.37</td>\n",
       "      <td>82.79</td>\n",
       "      <td>18.56</td>\n",
       "      <td>64.0</td>\n",
       "      <td>10.31</td>\n",
       "      <td>24.22</td>\n",
       "      <td>21.16</td>\n",
       "      <td>25.85</td>\n",
       "      <td>Llama-2-7b-chat-hf_1_4_4_2</td>\n",
       "      <td>26.485714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14.50</td>\n",
       "      <td>4.25</td>\n",
       "      <td>7.08</td>\n",
       "      <td>29.15</td>\n",
       "      <td>39.85</td>\n",
       "      <td>23.84</td>\n",
       "      <td>47.79</td>\n",
       "      <td>19.79</td>\n",
       "      <td>5.14</td>\n",
       "      <td>1.37</td>\n",
       "      <td>...</td>\n",
       "      <td>26.82</td>\n",
       "      <td>82.39</td>\n",
       "      <td>18.20</td>\n",
       "      <td>64.0</td>\n",
       "      <td>20.16</td>\n",
       "      <td>19.43</td>\n",
       "      <td>22.92</td>\n",
       "      <td>26.52</td>\n",
       "      <td>Llama-2-7b-chat-hf_1_4_1_2</td>\n",
       "      <td>25.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>18.75</td>\n",
       "      <td>5.50</td>\n",
       "      <td>3.00</td>\n",
       "      <td>36.34</td>\n",
       "      <td>40.29</td>\n",
       "      <td>25.58</td>\n",
       "      <td>50.75</td>\n",
       "      <td>21.06</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.23</td>\n",
       "      <td>...</td>\n",
       "      <td>17.87</td>\n",
       "      <td>82.83</td>\n",
       "      <td>17.21</td>\n",
       "      <td>64.0</td>\n",
       "      <td>10.01</td>\n",
       "      <td>21.68</td>\n",
       "      <td>24.06</td>\n",
       "      <td>25.94</td>\n",
       "      <td>Llama-2-7b-chat-hf_1_4_3_2</td>\n",
       "      <td>25.625714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18.00</td>\n",
       "      <td>5.75</td>\n",
       "      <td>5.25</td>\n",
       "      <td>35.82</td>\n",
       "      <td>40.84</td>\n",
       "      <td>25.94</td>\n",
       "      <td>48.77</td>\n",
       "      <td>20.79</td>\n",
       "      <td>2.07</td>\n",
       "      <td>0.20</td>\n",
       "      <td>...</td>\n",
       "      <td>17.86</td>\n",
       "      <td>82.38</td>\n",
       "      <td>16.97</td>\n",
       "      <td>64.0</td>\n",
       "      <td>11.52</td>\n",
       "      <td>20.49</td>\n",
       "      <td>22.02</td>\n",
       "      <td>25.16</td>\n",
       "      <td>Llama-2-7b-chat-hf_1_2_2_2</td>\n",
       "      <td>25.399048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>18.25</td>\n",
       "      <td>6.00</td>\n",
       "      <td>4.17</td>\n",
       "      <td>34.25</td>\n",
       "      <td>38.09</td>\n",
       "      <td>24.58</td>\n",
       "      <td>49.53</td>\n",
       "      <td>20.45</td>\n",
       "      <td>3.31</td>\n",
       "      <td>0.17</td>\n",
       "      <td>...</td>\n",
       "      <td>23.78</td>\n",
       "      <td>83.03</td>\n",
       "      <td>18.30</td>\n",
       "      <td>64.5</td>\n",
       "      <td>5.25</td>\n",
       "      <td>19.47</td>\n",
       "      <td>26.41</td>\n",
       "      <td>24.07</td>\n",
       "      <td>Llama-2-7b-chat-hf_1_4_2_2</td>\n",
       "      <td>25.372381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16.00</td>\n",
       "      <td>5.08</td>\n",
       "      <td>4.50</td>\n",
       "      <td>22.00</td>\n",
       "      <td>35.49</td>\n",
       "      <td>19.18</td>\n",
       "      <td>41.81</td>\n",
       "      <td>18.63</td>\n",
       "      <td>4.00</td>\n",
       "      <td>3.61</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>82.29</td>\n",
       "      <td>17.65</td>\n",
       "      <td>61.0</td>\n",
       "      <td>16.88</td>\n",
       "      <td>15.50</td>\n",
       "      <td>26.19</td>\n",
       "      <td>17.74</td>\n",
       "      <td>Llama-2-7b-chat-hf_1_2_1_2</td>\n",
       "      <td>23.232381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>18.75</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>32.44</td>\n",
       "      <td>36.93</td>\n",
       "      <td>23.49</td>\n",
       "      <td>43.15</td>\n",
       "      <td>20.16</td>\n",
       "      <td>2.50</td>\n",
       "      <td>0.22</td>\n",
       "      <td>...</td>\n",
       "      <td>17.21</td>\n",
       "      <td>82.01</td>\n",
       "      <td>14.94</td>\n",
       "      <td>64.5</td>\n",
       "      <td>6.11</td>\n",
       "      <td>13.83</td>\n",
       "      <td>18.61</td>\n",
       "      <td>19.29</td>\n",
       "      <td>Llama-2-7b-chat-hf_3_4_2_2</td>\n",
       "      <td>22.808095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>15.00</td>\n",
       "      <td>2.25</td>\n",
       "      <td>4.83</td>\n",
       "      <td>20.06</td>\n",
       "      <td>30.37</td>\n",
       "      <td>17.30</td>\n",
       "      <td>38.59</td>\n",
       "      <td>18.16</td>\n",
       "      <td>4.75</td>\n",
       "      <td>3.48</td>\n",
       "      <td>...</td>\n",
       "      <td>20.97</td>\n",
       "      <td>77.87</td>\n",
       "      <td>16.37</td>\n",
       "      <td>61.0</td>\n",
       "      <td>12.47</td>\n",
       "      <td>11.86</td>\n",
       "      <td>23.36</td>\n",
       "      <td>15.29</td>\n",
       "      <td>Llama-2-7b-chat-hf_3_4_1_2</td>\n",
       "      <td>21.263810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>15.00</td>\n",
       "      <td>2.88</td>\n",
       "      <td>2.52</td>\n",
       "      <td>16.55</td>\n",
       "      <td>28.89</td>\n",
       "      <td>15.63</td>\n",
       "      <td>35.78</td>\n",
       "      <td>18.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>4.64</td>\n",
       "      <td>...</td>\n",
       "      <td>11.82</td>\n",
       "      <td>77.51</td>\n",
       "      <td>12.93</td>\n",
       "      <td>62.5</td>\n",
       "      <td>12.26</td>\n",
       "      <td>11.53</td>\n",
       "      <td>19.65</td>\n",
       "      <td>14.00</td>\n",
       "      <td>Llama-2-7b-chat-hf_all_2</td>\n",
       "      <td>19.962857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     lsht  passage_retrieval_en  passage_retrieval_zh  multifieldqa_en  \\\n",
       "18  17.25                  8.50                 11.00            36.47   \n",
       "11  17.75                  8.50                  9.50            36.44   \n",
       "3   17.50                  8.00                 10.00            38.73   \n",
       "17  18.25                  8.00                  8.00            39.75   \n",
       "7   18.25                  9.00                  8.50            38.43   \n",
       "1   18.25                  9.00                  8.00            38.90   \n",
       "5   17.75                  8.00                  9.00            36.62   \n",
       "13  17.75                  8.50                  7.50            38.93   \n",
       "9   17.00                  8.00                  9.00            37.32   \n",
       "15  18.75                  7.00                  9.50            39.06   \n",
       "10  16.75                  8.50                 10.25            36.75   \n",
       "4   14.50                  4.25                  7.08            29.15   \n",
       "8   18.75                  5.50                  3.00            36.34   \n",
       "2   18.00                  5.75                  5.25            35.82   \n",
       "6   18.25                  6.00                  4.17            34.25   \n",
       "0   16.00                  5.08                  4.50            22.00   \n",
       "14  18.75                  2.00                  1.00            32.44   \n",
       "12  15.00                  2.25                  4.83            20.06   \n",
       "16  15.00                  2.88                  2.52            16.55   \n",
       "\n",
       "    samsum  multi_news  repobench-p  qmsum  passage_count  vcsum  ...  \\\n",
       "18   41.41       25.51        52.30  21.35           3.69   0.14  ...   \n",
       "11   41.36       25.70        52.25  20.93           3.79   0.13  ...   \n",
       "3    41.45       25.81        52.08  20.72           3.18   0.14  ...   \n",
       "17   41.51       25.35        51.76  20.48           3.99   0.14  ...   \n",
       "7    40.84       25.60        52.56  20.62           3.73   0.14  ...   \n",
       "1    41.25       25.33        51.91  20.69           4.11   0.15  ...   \n",
       "5    41.26       25.51        51.80  21.07           3.12   0.15  ...   \n",
       "13   41.75       25.06        52.10  20.45           4.06   0.15  ...   \n",
       "9    41.24       25.72        52.18  20.99           3.14   0.14  ...   \n",
       "15   40.95       25.69        51.89  20.51           3.45   0.14  ...   \n",
       "10   41.01       25.36        51.52  20.83           3.88   0.17  ...   \n",
       "4    39.85       23.84        47.79  19.79           5.14   1.37  ...   \n",
       "8    40.29       25.58        50.75  21.06           2.00   0.23  ...   \n",
       "2    40.84       25.94        48.77  20.79           2.07   0.20  ...   \n",
       "6    38.09       24.58        49.53  20.45           3.31   0.17  ...   \n",
       "0    35.49       19.18        41.81  18.63           4.00   3.61  ...   \n",
       "14   36.93       23.49        43.15  20.16           2.50   0.22  ...   \n",
       "12   30.37       17.30        38.59  18.16           4.75   3.48  ...   \n",
       "16   28.89       15.63        35.78  18.00           5.00   4.64  ...   \n",
       "\n",
       "    2wikimqa  triviaqa  narrativeqa  trec  multifieldqa_zh  qasper  hotpotqa  \\\n",
       "18     23.25     83.09        18.97  64.0             8.06   25.47     24.37   \n",
       "11     22.62     83.59        19.23  64.0             8.08   25.42     24.72   \n",
       "3      23.17     83.51        19.14  64.0             8.68   25.14     22.11   \n",
       "17     22.78     83.09        19.25  64.0             8.08   23.01     24.91   \n",
       "7      23.56     83.34        18.65  64.0             7.70   22.73     23.94   \n",
       "1      22.93     83.17        19.19  64.0             7.60   23.44     23.67   \n",
       "5      22.26     83.34        19.24  64.0             8.04   25.21     24.55   \n",
       "13     23.60     82.76        18.83  64.0             7.97   23.18     24.19   \n",
       "9      23.72     83.26        19.20  64.0             7.69   24.51     23.23   \n",
       "15     23.34     82.88        18.83  64.0             7.68   22.86     23.43   \n",
       "10     22.37     82.79        18.56  64.0            10.31   24.22     21.16   \n",
       "4      26.82     82.39        18.20  64.0            20.16   19.43     22.92   \n",
       "8      17.87     82.83        17.21  64.0            10.01   21.68     24.06   \n",
       "2      17.86     82.38        16.97  64.0            11.52   20.49     22.02   \n",
       "6      23.78     83.03        18.30  64.5             5.25   19.47     26.41   \n",
       "0      22.54     82.29        17.65  61.0            16.88   15.50     26.19   \n",
       "14     17.21     82.01        14.94  64.5             6.11   13.83     18.61   \n",
       "12     20.97     77.87        16.37  61.0            12.47   11.86     23.36   \n",
       "16     11.82     77.51        12.93  62.5            12.26   11.53     19.65   \n",
       "\n",
       "    gov_report                  experiment  avg_score  \n",
       "18       26.80   Llama-2-7b-chat-hf_zero_2  26.845238  \n",
       "11       26.35  Llama-2-7b-chat-hf_1_4_4_4  26.795714  \n",
       "3        26.19  Llama-2-7b-chat-hf_1_2_2_4  26.737619  \n",
       "17       26.63    Llama-2-7b-chat-hf_all_4  26.720952  \n",
       "7        26.80  Llama-2-7b-chat-hf_1_4_2_4  26.697143  \n",
       "1        26.43  Llama-2-7b-chat-hf_1_2_1_4  26.664762  \n",
       "5        26.69  Llama-2-7b-chat-hf_1_4_1_4  26.659048  \n",
       "13       26.68  Llama-2-7b-chat-hf_3_4_1_4  26.652381  \n",
       "9        26.52  Llama-2-7b-chat-hf_1_4_3_4  26.607143  \n",
       "15       26.43  Llama-2-7b-chat-hf_3_4_2_4  26.558095  \n",
       "10       25.85  Llama-2-7b-chat-hf_1_4_4_2  26.485714  \n",
       "4        26.52  Llama-2-7b-chat-hf_1_4_1_2  25.700000  \n",
       "8        25.94  Llama-2-7b-chat-hf_1_4_3_2  25.625714  \n",
       "2        25.16  Llama-2-7b-chat-hf_1_2_2_2  25.399048  \n",
       "6        24.07  Llama-2-7b-chat-hf_1_4_2_2  25.372381  \n",
       "0        17.74  Llama-2-7b-chat-hf_1_2_1_2  23.232381  \n",
       "14       19.29  Llama-2-7b-chat-hf_3_4_2_2  22.808095  \n",
       "12       15.29  Llama-2-7b-chat-hf_3_4_1_2  21.263810  \n",
       "16       14.00    Llama-2-7b-chat-hf_all_2  19.962857  \n",
       "\n",
       "[19 rows x 23 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "for experiment in experiments:\n",
    "    json_file = f\"pred/{experiment}/result.json\"\n",
    "    \n",
    "    with open(json_file) as f:\n",
    "        json_data = json.load(f)\n",
    "    json_data['experiment'] = experiment\n",
    "    data.append(json_data)\n",
    "\n",
    "data = pd.DataFrame(data)\n",
    "data['avg_score'] =  data.drop(columns = 'experiment').mean(axis = 1)\n",
    "avg_data = data.sort_values(by = 'avg_score', ascending = False)\n",
    "avg_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338b5324-8411-41df-8021-faf166dfcc80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bf9fa1-64d3-4cf0-9c60-7a9fade4d80c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b227ee6-153a-4379-9c74-a1c88408a948",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d3d54e0-ab30-40d2-bdf6-8271e784e011",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for experiment in experiments:\n",
    "    json_file = f\"pred/{experiment}/result.json\"\n",
    "    \n",
    "    with open(json_file) as f:\n",
    "        json_data = json.load(f)\n",
    "    json_data['experiment'] = experiment\n",
    "    data.append(json_data)\n",
    "\n",
    "data = pd.DataFrame(data)\n",
    "data['avg_score'] =  data.drop(columns = 'experiment').mean(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "126e2e7e-8d51-46e1-8a0a-cfe8a0ca53c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>layer_strategy</th>\n",
       "      <th>layer_set</th>\n",
       "      <th>num_bits</th>\n",
       "      <th>experiment_description</th>\n",
       "      <th>lsht</th>\n",
       "      <th>passage_retrieval_en</th>\n",
       "      <th>passage_retrieval_zh</th>\n",
       "      <th>multifieldqa_en</th>\n",
       "      <th>samsum</th>\n",
       "      <th>multi_news</th>\n",
       "      <th>...</th>\n",
       "      <th>2wikimqa</th>\n",
       "      <th>triviaqa</th>\n",
       "      <th>narrativeqa</th>\n",
       "      <th>trec</th>\n",
       "      <th>multifieldqa_zh</th>\n",
       "      <th>qasper</th>\n",
       "      <th>hotpotqa</th>\n",
       "      <th>gov_report</th>\n",
       "      <th>avg_score</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1_2</td>\n",
       "      <td>first quarter</td>\n",
       "      <td>2</td>\n",
       "      <td>Model with 1_2 of layers (first quarter) quant...</td>\n",
       "      <td>16.00</td>\n",
       "      <td>5.08</td>\n",
       "      <td>4.50</td>\n",
       "      <td>22.00</td>\n",
       "      <td>35.49</td>\n",
       "      <td>19.18</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>82.29</td>\n",
       "      <td>17.65</td>\n",
       "      <td>61.0</td>\n",
       "      <td>16.88</td>\n",
       "      <td>15.50</td>\n",
       "      <td>26.19</td>\n",
       "      <td>17.74</td>\n",
       "      <td>23.232381</td>\n",
       "      <td>Llama-2-7b-chat-hf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1_2</td>\n",
       "      <td>first quarter</td>\n",
       "      <td>4</td>\n",
       "      <td>Model with 1_2 of layers (first quarter) quant...</td>\n",
       "      <td>18.25</td>\n",
       "      <td>9.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>38.90</td>\n",
       "      <td>41.25</td>\n",
       "      <td>25.33</td>\n",
       "      <td>...</td>\n",
       "      <td>22.93</td>\n",
       "      <td>83.17</td>\n",
       "      <td>19.19</td>\n",
       "      <td>64.0</td>\n",
       "      <td>7.60</td>\n",
       "      <td>23.44</td>\n",
       "      <td>23.67</td>\n",
       "      <td>26.43</td>\n",
       "      <td>26.664762</td>\n",
       "      <td>Llama-2-7b-chat-hf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1_2</td>\n",
       "      <td>second quarter</td>\n",
       "      <td>2</td>\n",
       "      <td>Model with 1_2 of layers (second quarter) quan...</td>\n",
       "      <td>18.00</td>\n",
       "      <td>5.75</td>\n",
       "      <td>5.25</td>\n",
       "      <td>35.82</td>\n",
       "      <td>40.84</td>\n",
       "      <td>25.94</td>\n",
       "      <td>...</td>\n",
       "      <td>17.86</td>\n",
       "      <td>82.38</td>\n",
       "      <td>16.97</td>\n",
       "      <td>64.0</td>\n",
       "      <td>11.52</td>\n",
       "      <td>20.49</td>\n",
       "      <td>22.02</td>\n",
       "      <td>25.16</td>\n",
       "      <td>25.399048</td>\n",
       "      <td>Llama-2-7b-chat-hf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1_2</td>\n",
       "      <td>second quarter</td>\n",
       "      <td>4</td>\n",
       "      <td>Model with 1_2 of layers (second quarter) quan...</td>\n",
       "      <td>17.50</td>\n",
       "      <td>8.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>38.73</td>\n",
       "      <td>41.45</td>\n",
       "      <td>25.81</td>\n",
       "      <td>...</td>\n",
       "      <td>23.17</td>\n",
       "      <td>83.51</td>\n",
       "      <td>19.14</td>\n",
       "      <td>64.0</td>\n",
       "      <td>8.68</td>\n",
       "      <td>25.14</td>\n",
       "      <td>22.11</td>\n",
       "      <td>26.19</td>\n",
       "      <td>26.737619</td>\n",
       "      <td>Llama-2-7b-chat-hf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1_4</td>\n",
       "      <td>first quarter</td>\n",
       "      <td>2</td>\n",
       "      <td>Model with 1_4 of layers (first quarter) quant...</td>\n",
       "      <td>14.50</td>\n",
       "      <td>4.25</td>\n",
       "      <td>7.08</td>\n",
       "      <td>29.15</td>\n",
       "      <td>39.85</td>\n",
       "      <td>23.84</td>\n",
       "      <td>...</td>\n",
       "      <td>26.82</td>\n",
       "      <td>82.39</td>\n",
       "      <td>18.20</td>\n",
       "      <td>64.0</td>\n",
       "      <td>20.16</td>\n",
       "      <td>19.43</td>\n",
       "      <td>22.92</td>\n",
       "      <td>26.52</td>\n",
       "      <td>25.700000</td>\n",
       "      <td>Llama-2-7b-chat-hf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  layer_strategy       layer_set num_bits  \\\n",
       "0            1_2   first quarter        2   \n",
       "1            1_2   first quarter        4   \n",
       "2            1_2  second quarter        2   \n",
       "3            1_2  second quarter        4   \n",
       "4            1_4   first quarter        2   \n",
       "\n",
       "                              experiment_description   lsht  \\\n",
       "0  Model with 1_2 of layers (first quarter) quant...  16.00   \n",
       "1  Model with 1_2 of layers (first quarter) quant...  18.25   \n",
       "2  Model with 1_2 of layers (second quarter) quan...  18.00   \n",
       "3  Model with 1_2 of layers (second quarter) quan...  17.50   \n",
       "4  Model with 1_4 of layers (first quarter) quant...  14.50   \n",
       "\n",
       "   passage_retrieval_en  passage_retrieval_zh  multifieldqa_en  samsum  \\\n",
       "0                  5.08                  4.50            22.00   35.49   \n",
       "1                  9.00                  8.00            38.90   41.25   \n",
       "2                  5.75                  5.25            35.82   40.84   \n",
       "3                  8.00                 10.00            38.73   41.45   \n",
       "4                  4.25                  7.08            29.15   39.85   \n",
       "\n",
       "   multi_news  ...  2wikimqa  triviaqa  narrativeqa  trec  multifieldqa_zh  \\\n",
       "0       19.18  ...     22.54     82.29        17.65  61.0            16.88   \n",
       "1       25.33  ...     22.93     83.17        19.19  64.0             7.60   \n",
       "2       25.94  ...     17.86     82.38        16.97  64.0            11.52   \n",
       "3       25.81  ...     23.17     83.51        19.14  64.0             8.68   \n",
       "4       23.84  ...     26.82     82.39        18.20  64.0            20.16   \n",
       "\n",
       "   qasper  hotpotqa  gov_report  avg_score               model  \n",
       "0   15.50     26.19       17.74  23.232381  Llama-2-7b-chat-hf  \n",
       "1   23.44     23.67       26.43  26.664762  Llama-2-7b-chat-hf  \n",
       "2   20.49     22.02       25.16  25.399048  Llama-2-7b-chat-hf  \n",
       "3   25.14     22.11       26.19  26.737619  Llama-2-7b-chat-hf  \n",
       "4   19.43     22.92       26.52  25.700000  Llama-2-7b-chat-hf  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def map_experiment(experiment):\n",
    "    parts = experiment.split('_')\n",
    "    base_model = 'Llama-2-7b-chat-hf'\n",
    "    \n",
    "    if 'zero' in experiment:\n",
    "        return {\n",
    "            'layer_strategy': 'zero',\n",
    "            'layer_set': 'NA',\n",
    "            'num_bits': 'NA',\n",
    "            'experiment_description': f'Model with no quantization'\n",
    "        }\n",
    "    elif 'all' in experiment:\n",
    "        return {\n",
    "            'layer_strategy': 'all',\n",
    "            'layer_set': 'NA',\n",
    "            'num_bits': parts[-1],\n",
    "            'experiment_description': f'Model with all layers quantized to {parts[-1]} bits'\n",
    "        }\n",
    "    else:\n",
    "        layer_strategy = f\"{parts[-4]}_{parts[-3]}\"\n",
    "        layer_set = int(parts[-2])\n",
    "        num_bits = parts[-1]\n",
    "        \n",
    "        layer_set_description = {\n",
    "            1: 'first quarter',\n",
    "            2: 'second quarter',\n",
    "            3: 'third quarter',\n",
    "            4: 'fourth quarter'\n",
    "        }\n",
    "        \n",
    "        return {\n",
    "            'layer_strategy': layer_strategy,\n",
    "            'layer_set': layer_set_description[layer_set],\n",
    "            'num_bits': num_bits,\n",
    "            'experiment_description': f'Model with {layer_strategy} of layers ({layer_set_description[layer_set]}) quantized to {num_bits} bits'\n",
    "        }\n",
    "\n",
    "data['model'] = data.experiment.apply(lambda x: x.split(\"_\")[0])\n",
    "data['experiment'] = data['experiment'].apply(map_experiment)\n",
    "data[['layer_strategy', 'layer_set', 'num_bits', 'experiment_description']] = pd.DataFrame(data['experiment'].tolist(), index=data.index)\n",
    "data = data.drop(columns = ['experiment'])\n",
    "\n",
    "cols = data.columns.tolist()\n",
    "cols = cols[-4:] + cols[:-4]\n",
    "data = data[cols]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a701b4f-aaf2-4a0f-863a-b6927668c8bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>layer_strategy</th>\n",
       "      <th>num_bits</th>\n",
       "      <th>layer_set</th>\n",
       "      <th>avg_score</th>\n",
       "      <th>score_diff</th>\n",
       "      <th>cumulative_score_diff</th>\n",
       "      <th>score_percent_diff</th>\n",
       "      <th>cumulative_score_percent_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Llama-2-7b-chat-hf</td>\n",
       "      <td>zero</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>26.845238</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.18%</td>\n",
       "      <td>0.18%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Llama-2-7b-chat-hf</td>\n",
       "      <td>1_4</td>\n",
       "      <td>4</td>\n",
       "      <td>fourth quarter</td>\n",
       "      <td>26.795714</td>\n",
       "      <td>0.049524</td>\n",
       "      <td>0.049524</td>\n",
       "      <td>0.22%</td>\n",
       "      <td>0.40%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Llama-2-7b-chat-hf</td>\n",
       "      <td>1_2</td>\n",
       "      <td>4</td>\n",
       "      <td>second quarter</td>\n",
       "      <td>26.737619</td>\n",
       "      <td>0.058095</td>\n",
       "      <td>0.107619</td>\n",
       "      <td>0.06%</td>\n",
       "      <td>0.47%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Llama-2-7b-chat-hf</td>\n",
       "      <td>all</td>\n",
       "      <td>4</td>\n",
       "      <td>NA</td>\n",
       "      <td>26.720952</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.124286</td>\n",
       "      <td>0.09%</td>\n",
       "      <td>0.55%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Llama-2-7b-chat-hf</td>\n",
       "      <td>1_4</td>\n",
       "      <td>4</td>\n",
       "      <td>second quarter</td>\n",
       "      <td>26.697143</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.148095</td>\n",
       "      <td>0.12%</td>\n",
       "      <td>0.68%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Llama-2-7b-chat-hf</td>\n",
       "      <td>1_2</td>\n",
       "      <td>4</td>\n",
       "      <td>first quarter</td>\n",
       "      <td>26.664762</td>\n",
       "      <td>0.032381</td>\n",
       "      <td>0.180476</td>\n",
       "      <td>0.02%</td>\n",
       "      <td>0.70%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Llama-2-7b-chat-hf</td>\n",
       "      <td>1_4</td>\n",
       "      <td>4</td>\n",
       "      <td>first quarter</td>\n",
       "      <td>26.659048</td>\n",
       "      <td>0.005714</td>\n",
       "      <td>0.186190</td>\n",
       "      <td>0.03%</td>\n",
       "      <td>0.72%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Llama-2-7b-chat-hf</td>\n",
       "      <td>3_4</td>\n",
       "      <td>4</td>\n",
       "      <td>first quarter</td>\n",
       "      <td>26.652381</td>\n",
       "      <td>0.006667</td>\n",
       "      <td>0.192857</td>\n",
       "      <td>0.17%</td>\n",
       "      <td>0.89%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Llama-2-7b-chat-hf</td>\n",
       "      <td>1_4</td>\n",
       "      <td>4</td>\n",
       "      <td>third quarter</td>\n",
       "      <td>26.607143</td>\n",
       "      <td>0.045238</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.18%</td>\n",
       "      <td>1.08%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Llama-2-7b-chat-hf</td>\n",
       "      <td>3_4</td>\n",
       "      <td>4</td>\n",
       "      <td>second quarter</td>\n",
       "      <td>26.558095</td>\n",
       "      <td>0.049048</td>\n",
       "      <td>0.287143</td>\n",
       "      <td>0.27%</td>\n",
       "      <td>1.36%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Llama-2-7b-chat-hf</td>\n",
       "      <td>1_4</td>\n",
       "      <td>2</td>\n",
       "      <td>fourth quarter</td>\n",
       "      <td>26.485714</td>\n",
       "      <td>0.072381</td>\n",
       "      <td>0.359524</td>\n",
       "      <td>3.06%</td>\n",
       "      <td>4.46%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Llama-2-7b-chat-hf</td>\n",
       "      <td>1_4</td>\n",
       "      <td>2</td>\n",
       "      <td>first quarter</td>\n",
       "      <td>25.700000</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>1.145238</td>\n",
       "      <td>0.29%</td>\n",
       "      <td>4.76%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Llama-2-7b-chat-hf</td>\n",
       "      <td>1_4</td>\n",
       "      <td>2</td>\n",
       "      <td>third quarter</td>\n",
       "      <td>25.625714</td>\n",
       "      <td>0.074286</td>\n",
       "      <td>1.219524</td>\n",
       "      <td>0.89%</td>\n",
       "      <td>5.69%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Llama-2-7b-chat-hf</td>\n",
       "      <td>1_2</td>\n",
       "      <td>2</td>\n",
       "      <td>second quarter</td>\n",
       "      <td>25.399048</td>\n",
       "      <td>0.226667</td>\n",
       "      <td>1.446190</td>\n",
       "      <td>0.11%</td>\n",
       "      <td>5.80%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Llama-2-7b-chat-hf</td>\n",
       "      <td>1_4</td>\n",
       "      <td>2</td>\n",
       "      <td>second quarter</td>\n",
       "      <td>25.372381</td>\n",
       "      <td>0.026667</td>\n",
       "      <td>1.472857</td>\n",
       "      <td>9.21%</td>\n",
       "      <td>15.55%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Llama-2-7b-chat-hf</td>\n",
       "      <td>1_2</td>\n",
       "      <td>2</td>\n",
       "      <td>first quarter</td>\n",
       "      <td>23.232381</td>\n",
       "      <td>2.140000</td>\n",
       "      <td>3.612857</td>\n",
       "      <td>1.86%</td>\n",
       "      <td>17.70%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Llama-2-7b-chat-hf</td>\n",
       "      <td>3_4</td>\n",
       "      <td>2</td>\n",
       "      <td>second quarter</td>\n",
       "      <td>22.808095</td>\n",
       "      <td>0.424286</td>\n",
       "      <td>4.037143</td>\n",
       "      <td>7.26%</td>\n",
       "      <td>26.25%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Llama-2-7b-chat-hf</td>\n",
       "      <td>3_4</td>\n",
       "      <td>2</td>\n",
       "      <td>first quarter</td>\n",
       "      <td>21.263810</td>\n",
       "      <td>1.544286</td>\n",
       "      <td>5.581429</td>\n",
       "      <td>6.52%</td>\n",
       "      <td>34.48%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Llama-2-7b-chat-hf</td>\n",
       "      <td>all</td>\n",
       "      <td>2</td>\n",
       "      <td>NA</td>\n",
       "      <td>19.962857</td>\n",
       "      <td>1.300952</td>\n",
       "      <td>6.882381</td>\n",
       "      <td>nan%</td>\n",
       "      <td>nan%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 model layer_strategy num_bits       layer_set  avg_score  \\\n",
       "18  Llama-2-7b-chat-hf           zero       NA              NA  26.845238   \n",
       "11  Llama-2-7b-chat-hf            1_4        4  fourth quarter  26.795714   \n",
       "3   Llama-2-7b-chat-hf            1_2        4  second quarter  26.737619   \n",
       "17  Llama-2-7b-chat-hf            all        4              NA  26.720952   \n",
       "7   Llama-2-7b-chat-hf            1_4        4  second quarter  26.697143   \n",
       "1   Llama-2-7b-chat-hf            1_2        4   first quarter  26.664762   \n",
       "5   Llama-2-7b-chat-hf            1_4        4   first quarter  26.659048   \n",
       "13  Llama-2-7b-chat-hf            3_4        4   first quarter  26.652381   \n",
       "9   Llama-2-7b-chat-hf            1_4        4   third quarter  26.607143   \n",
       "15  Llama-2-7b-chat-hf            3_4        4  second quarter  26.558095   \n",
       "10  Llama-2-7b-chat-hf            1_4        2  fourth quarter  26.485714   \n",
       "4   Llama-2-7b-chat-hf            1_4        2   first quarter  25.700000   \n",
       "8   Llama-2-7b-chat-hf            1_4        2   third quarter  25.625714   \n",
       "2   Llama-2-7b-chat-hf            1_2        2  second quarter  25.399048   \n",
       "6   Llama-2-7b-chat-hf            1_4        2  second quarter  25.372381   \n",
       "0   Llama-2-7b-chat-hf            1_2        2   first quarter  23.232381   \n",
       "14  Llama-2-7b-chat-hf            3_4        2  second quarter  22.808095   \n",
       "12  Llama-2-7b-chat-hf            3_4        2   first quarter  21.263810   \n",
       "16  Llama-2-7b-chat-hf            all        2              NA  19.962857   \n",
       "\n",
       "    score_diff  cumulative_score_diff score_percent_diff  \\\n",
       "18         NaN                    NaN              0.18%   \n",
       "11    0.049524               0.049524              0.22%   \n",
       "3     0.058095               0.107619              0.06%   \n",
       "17    0.016667               0.124286              0.09%   \n",
       "7     0.023810               0.148095              0.12%   \n",
       "1     0.032381               0.180476              0.02%   \n",
       "5     0.005714               0.186190              0.03%   \n",
       "13    0.006667               0.192857              0.17%   \n",
       "9     0.045238               0.238095              0.18%   \n",
       "15    0.049048               0.287143              0.27%   \n",
       "10    0.072381               0.359524              3.06%   \n",
       "4     0.785714               1.145238              0.29%   \n",
       "8     0.074286               1.219524              0.89%   \n",
       "2     0.226667               1.446190              0.11%   \n",
       "6     0.026667               1.472857              9.21%   \n",
       "0     2.140000               3.612857              1.86%   \n",
       "14    0.424286               4.037143              7.26%   \n",
       "12    1.544286               5.581429              6.52%   \n",
       "16    1.300952               6.882381               nan%   \n",
       "\n",
       "   cumulative_score_percent_diff  \n",
       "18                         0.18%  \n",
       "11                         0.40%  \n",
       "3                          0.47%  \n",
       "17                         0.55%  \n",
       "7                          0.68%  \n",
       "1                          0.70%  \n",
       "5                          0.72%  \n",
       "13                         0.89%  \n",
       "9                          1.08%  \n",
       "15                         1.36%  \n",
       "10                         4.46%  \n",
       "4                          4.76%  \n",
       "8                          5.69%  \n",
       "2                          5.80%  \n",
       "6                         15.55%  \n",
       "0                         17.70%  \n",
       "14                        26.25%  \n",
       "12                        34.48%  \n",
       "16                          nan%  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_data = data.sort_values(by = 'avg_score', ascending = False)[['model', 'layer_strategy', 'num_bits', 'layer_set','avg_score']]\n",
    "avg_data['score_diff'] = avg_data['avg_score'].shift(1) - avg_data['avg_score']\n",
    "avg_data['cumulative_score_diff'] = avg_data['score_diff'].cumsum()\n",
    "\n",
    "avg_data['score_percent_diff'] = avg_data['avg_score'] / avg_data['avg_score'].shift(-1) - 1\n",
    "avg_data['cumulative_score_percent_diff'] = (1 + avg_data['score_percent_diff']).cumprod() - 1\n",
    "\n",
    "avg_data['score_percent_diff'] = avg_data['score_percent_diff'].apply(lambda x: f\"{x:.2%}\")\n",
    "avg_data['cumulative_score_percent_diff'] = avg_data['cumulative_score_percent_diff'].apply(lambda x: f\"{x:.2%}\")\n",
    "avg_data\n",
    "#avg_data.to_csv('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "036a8cbf-cd67-42a5-a637-97f204b92799",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>layer_strategy</th>\n",
       "      <th>num_bits</th>\n",
       "      <th>layer_set</th>\n",
       "      <th>avg_score</th>\n",
       "      <th>score_diff</th>\n",
       "      <th>cumulative_score_diff</th>\n",
       "      <th>score_percent_diff</th>\n",
       "      <th>cumulative_score_percent_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Llama-2-7b-chat-hf</td>\n",
       "      <td>zero</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>26.845238</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan%</td>\n",
       "      <td>nan%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Llama-2-7b-chat-hf</td>\n",
       "      <td>1_4</td>\n",
       "      <td>4</td>\n",
       "      <td>fourth quarter</td>\n",
       "      <td>26.795714</td>\n",
       "      <td>0.049524</td>\n",
       "      <td>0.049524</td>\n",
       "      <td>0.18%</td>\n",
       "      <td>0.18%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Llama-2-7b-chat-hf</td>\n",
       "      <td>1_2</td>\n",
       "      <td>4</td>\n",
       "      <td>second quarter</td>\n",
       "      <td>26.737619</td>\n",
       "      <td>0.058095</td>\n",
       "      <td>0.107619</td>\n",
       "      <td>0.22%</td>\n",
       "      <td>0.40%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Llama-2-7b-chat-hf</td>\n",
       "      <td>all</td>\n",
       "      <td>4</td>\n",
       "      <td>NA</td>\n",
       "      <td>26.720952</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.124286</td>\n",
       "      <td>0.06%</td>\n",
       "      <td>0.47%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Llama-2-7b-chat-hf</td>\n",
       "      <td>1_4</td>\n",
       "      <td>4</td>\n",
       "      <td>second quarter</td>\n",
       "      <td>26.697143</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.148095</td>\n",
       "      <td>0.09%</td>\n",
       "      <td>0.55%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 model layer_strategy num_bits       layer_set  avg_score  \\\n",
       "18  Llama-2-7b-chat-hf           zero       NA              NA  26.845238   \n",
       "11  Llama-2-7b-chat-hf            1_4        4  fourth quarter  26.795714   \n",
       "3   Llama-2-7b-chat-hf            1_2        4  second quarter  26.737619   \n",
       "17  Llama-2-7b-chat-hf            all        4              NA  26.720952   \n",
       "7   Llama-2-7b-chat-hf            1_4        4  second quarter  26.697143   \n",
       "\n",
       "    score_diff  cumulative_score_diff score_percent_diff  \\\n",
       "18         NaN                    NaN               nan%   \n",
       "11    0.049524               0.049524              0.18%   \n",
       "3     0.058095               0.107619              0.22%   \n",
       "17    0.016667               0.124286              0.06%   \n",
       "7     0.023810               0.148095              0.09%   \n",
       "\n",
       "   cumulative_score_percent_diff  \n",
       "18                          nan%  \n",
       "11                         0.18%  \n",
       "3                          0.40%  \n",
       "17                         0.47%  \n",
       "7                          0.55%  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_data = data.sort_values(by='avg_score', ascending=False)[['model', 'layer_strategy', 'num_bits', 'layer_set', 'avg_score']]\n",
    "\n",
    "# Calculate score differences using the updated logic\n",
    "avg_data['score_diff'] = avg_data['avg_score'].shift(1) - avg_data['avg_score']\n",
    "avg_data['cumulative_score_diff'] = avg_data['score_diff'].cumsum()\n",
    "\n",
    "# Calculate percentage differences using the updated logic\n",
    "avg_data['score_percent_diff'] = avg_data['avg_score'].shift(1) / avg_data['avg_score'] - 1\n",
    "avg_data['cumulative_score_percent_diff'] = (1 + avg_data['score_percent_diff']).cumprod() - 1\n",
    "\n",
    "# Format percentage differences\n",
    "avg_data['score_percent_diff'] = avg_data['score_percent_diff'].apply(lambda x: f\"{x:.2%}\")\n",
    "avg_data['cumulative_score_percent_diff'] = avg_data['cumulative_score_percent_diff'].apply(lambda x: f\"{x:.2%}\")\n",
    "\n",
    "avg_data.to_csv(\"performance_summary_long_bench.csv\", index = False)\n",
    "avg_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30e9d4e2-cb28-4558-b246-347d84162f7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>layer_strategy</th>\n",
       "      <th>layer_set</th>\n",
       "      <th>num_bits</th>\n",
       "      <th>experiment_description</th>\n",
       "      <th>lsht</th>\n",
       "      <th>passage_retrieval_en</th>\n",
       "      <th>passage_retrieval_zh</th>\n",
       "      <th>multifieldqa_en</th>\n",
       "      <th>samsum</th>\n",
       "      <th>multi_news</th>\n",
       "      <th>...</th>\n",
       "      <th>multifieldqa_zh</th>\n",
       "      <th>qasper</th>\n",
       "      <th>hotpotqa</th>\n",
       "      <th>gov_report</th>\n",
       "      <th>avg_score</th>\n",
       "      <th>model</th>\n",
       "      <th>score_diff</th>\n",
       "      <th>cumulative_score_diff</th>\n",
       "      <th>score_percent_diff</th>\n",
       "      <th>cumulative_score_percent_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>zero</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>Model with no quantization</td>\n",
       "      <td>17.25</td>\n",
       "      <td>8.5</td>\n",
       "      <td>11.0</td>\n",
       "      <td>36.47</td>\n",
       "      <td>41.41</td>\n",
       "      <td>25.51</td>\n",
       "      <td>...</td>\n",
       "      <td>8.06</td>\n",
       "      <td>25.47</td>\n",
       "      <td>24.37</td>\n",
       "      <td>26.80</td>\n",
       "      <td>26.845238</td>\n",
       "      <td>Llama-2-7b-chat-hf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan%</td>\n",
       "      <td>nan%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1_4</td>\n",
       "      <td>fourth quarter</td>\n",
       "      <td>4</td>\n",
       "      <td>Model with 1_4 of layers (fourth quarter) quan...</td>\n",
       "      <td>17.75</td>\n",
       "      <td>8.5</td>\n",
       "      <td>9.5</td>\n",
       "      <td>36.44</td>\n",
       "      <td>41.36</td>\n",
       "      <td>25.70</td>\n",
       "      <td>...</td>\n",
       "      <td>8.08</td>\n",
       "      <td>25.42</td>\n",
       "      <td>24.72</td>\n",
       "      <td>26.35</td>\n",
       "      <td>26.795714</td>\n",
       "      <td>Llama-2-7b-chat-hf</td>\n",
       "      <td>0.049524</td>\n",
       "      <td>0.049524</td>\n",
       "      <td>0.18%</td>\n",
       "      <td>0.18%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1_2</td>\n",
       "      <td>second quarter</td>\n",
       "      <td>4</td>\n",
       "      <td>Model with 1_2 of layers (second quarter) quan...</td>\n",
       "      <td>17.50</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>38.73</td>\n",
       "      <td>41.45</td>\n",
       "      <td>25.81</td>\n",
       "      <td>...</td>\n",
       "      <td>8.68</td>\n",
       "      <td>25.14</td>\n",
       "      <td>22.11</td>\n",
       "      <td>26.19</td>\n",
       "      <td>26.737619</td>\n",
       "      <td>Llama-2-7b-chat-hf</td>\n",
       "      <td>0.058095</td>\n",
       "      <td>0.107619</td>\n",
       "      <td>0.22%</td>\n",
       "      <td>0.40%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>all</td>\n",
       "      <td>NA</td>\n",
       "      <td>4</td>\n",
       "      <td>Model with all layers quantized to 4 bits</td>\n",
       "      <td>18.25</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>39.75</td>\n",
       "      <td>41.51</td>\n",
       "      <td>25.35</td>\n",
       "      <td>...</td>\n",
       "      <td>8.08</td>\n",
       "      <td>23.01</td>\n",
       "      <td>24.91</td>\n",
       "      <td>26.63</td>\n",
       "      <td>26.720952</td>\n",
       "      <td>Llama-2-7b-chat-hf</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.124286</td>\n",
       "      <td>0.06%</td>\n",
       "      <td>0.47%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1_4</td>\n",
       "      <td>second quarter</td>\n",
       "      <td>4</td>\n",
       "      <td>Model with 1_4 of layers (second quarter) quan...</td>\n",
       "      <td>18.25</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.5</td>\n",
       "      <td>38.43</td>\n",
       "      <td>40.84</td>\n",
       "      <td>25.60</td>\n",
       "      <td>...</td>\n",
       "      <td>7.70</td>\n",
       "      <td>22.73</td>\n",
       "      <td>23.94</td>\n",
       "      <td>26.80</td>\n",
       "      <td>26.697143</td>\n",
       "      <td>Llama-2-7b-chat-hf</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.148095</td>\n",
       "      <td>0.09%</td>\n",
       "      <td>0.55%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   layer_strategy       layer_set num_bits  \\\n",
       "18           zero              NA       NA   \n",
       "11            1_4  fourth quarter        4   \n",
       "3             1_2  second quarter        4   \n",
       "17            all              NA        4   \n",
       "7             1_4  second quarter        4   \n",
       "\n",
       "                               experiment_description   lsht  \\\n",
       "18                         Model with no quantization  17.25   \n",
       "11  Model with 1_4 of layers (fourth quarter) quan...  17.75   \n",
       "3   Model with 1_2 of layers (second quarter) quan...  17.50   \n",
       "17          Model with all layers quantized to 4 bits  18.25   \n",
       "7   Model with 1_4 of layers (second quarter) quan...  18.25   \n",
       "\n",
       "    passage_retrieval_en  passage_retrieval_zh  multifieldqa_en  samsum  \\\n",
       "18                   8.5                  11.0            36.47   41.41   \n",
       "11                   8.5                   9.5            36.44   41.36   \n",
       "3                    8.0                  10.0            38.73   41.45   \n",
       "17                   8.0                   8.0            39.75   41.51   \n",
       "7                    9.0                   8.5            38.43   40.84   \n",
       "\n",
       "    multi_news  ...  multifieldqa_zh  qasper  hotpotqa  gov_report  avg_score  \\\n",
       "18       25.51  ...             8.06   25.47     24.37       26.80  26.845238   \n",
       "11       25.70  ...             8.08   25.42     24.72       26.35  26.795714   \n",
       "3        25.81  ...             8.68   25.14     22.11       26.19  26.737619   \n",
       "17       25.35  ...             8.08   23.01     24.91       26.63  26.720952   \n",
       "7        25.60  ...             7.70   22.73     23.94       26.80  26.697143   \n",
       "\n",
       "                 model  score_diff  cumulative_score_diff  score_percent_diff  \\\n",
       "18  Llama-2-7b-chat-hf         NaN                    NaN                nan%   \n",
       "11  Llama-2-7b-chat-hf    0.049524               0.049524               0.18%   \n",
       "3   Llama-2-7b-chat-hf    0.058095               0.107619               0.22%   \n",
       "17  Llama-2-7b-chat-hf    0.016667               0.124286               0.06%   \n",
       "7   Llama-2-7b-chat-hf    0.023810               0.148095               0.09%   \n",
       "\n",
       "    cumulative_score_percent_diff  \n",
       "18                           nan%  \n",
       "11                          0.18%  \n",
       "3                           0.40%  \n",
       "17                          0.47%  \n",
       "7                           0.55%  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data = data.sort_values(by='avg_score', ascending=False)\n",
    "\n",
    "# Calculate score differences using the updated logic\n",
    "full_data['score_diff'] = full_data['avg_score'].shift(1) - full_data['avg_score']\n",
    "full_data['cumulative_score_diff'] = full_data['score_diff'].cumsum()\n",
    "\n",
    "# Calculate percentage differences using the updated logic\n",
    "full_data['score_percent_diff'] = full_data['avg_score'].shift(1) / full_data['avg_score'] - 1\n",
    "full_data['cumulative_score_percent_diff'] = (1 + full_data['score_percent_diff']).cumprod() - 1\n",
    "\n",
    "# Format percentage differences\n",
    "full_data['score_percent_diff'] = full_data['score_percent_diff'].apply(lambda x: f\"{x:.2%}\")\n",
    "full_data['cumulative_score_percent_diff'] = full_data['cumulative_score_percent_diff'].apply(lambda x: f\"{x:.2%}\")\n",
    "\n",
    "full_data.to_csv(\"performance_detailed_long_bench.csv\", index = False)\n",
    "full_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "dc48e9dc-3c5e-45a7-aca8-60c6a3975f6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19, 9)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfecb96-9d2b-42e3-804f-fc7f387a1892",
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e1968f-041b-4831-9c0c-158b14429178",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5acf8e-42fd-4713-b24f-a819a44747d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12440848-a646-47e8-b77e-b81fc74521b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97b7b49-5353-4f85-af77-58ed32e7b523",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
