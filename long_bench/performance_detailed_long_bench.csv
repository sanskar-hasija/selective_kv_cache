layer_strategy,layer_set,num_bits,experiment_description,lsht,passage_retrieval_en,passage_retrieval_zh,multifieldqa_en,samsum,multi_news,repobench-p,qmsum,passage_count,vcsum,lcc,dureader,musique,2wikimqa,triviaqa,narrativeqa,trec,multifieldqa_zh,qasper,hotpotqa,gov_report,avg_score,model,score_diff,cumulative_score_diff,score_percent_diff,cumulative_score_percent_diff
zero,NA,NA,Model with no quantization,17.25,8.5,11.0,36.47,41.41,25.51,52.3,21.35,3.69,0.14,58.34,5.27,8.51,23.25,83.09,18.97,64.0,8.06,25.47,24.37,26.8,26.84523809523809,Llama-2-7b-chat-hf,,,nan%,nan%
1_4,fourth quarter,4,Model with 1_4 of layers (fourth quarter) quantized to 4 bits,17.75,8.5,9.5,36.44,41.36,25.7,52.25,20.93,3.79,0.13,58.13,5.65,8.57,22.62,83.59,19.23,64.0,8.08,25.42,24.72,26.35,26.795714285714286,Llama-2-7b-chat-hf,0.049523809523805085,0.049523809523805085,0.18%,0.18%
1_2,second quarter,4,Model with 1_2 of layers (second quarter) quantized to 4 bits,17.5,8.0,10.0,38.73,41.45,25.81,52.08,20.72,3.18,0.14,58.16,5.45,8.33,23.17,83.51,19.14,64.0,8.68,25.14,22.11,26.19,26.73761904761905,Llama-2-7b-chat-hf,0.058095238095237534,0.10761904761904262,0.22%,0.40%
all,NA,4,Model with all layers quantized to 4 bits,18.25,8.0,8.0,39.75,41.51,25.35,51.76,20.48,3.99,0.14,58.23,5.58,8.35,22.78,83.09,19.25,64.0,8.08,23.01,24.91,26.63,26.72095238095238,Llama-2-7b-chat-hf,0.016666666666669272,0.12428571428571189,0.06%,0.47%
1_4,second quarter,4,Model with 1_4 of layers (second quarter) quantized to 4 bits,18.25,9.0,8.5,38.43,40.84,25.6,52.56,20.62,3.73,0.14,57.97,5.6,8.68,23.56,83.34,18.65,64.0,7.7,22.73,23.94,26.8,26.697142857142858,Llama-2-7b-chat-hf,0.02380952380952195,0.14809523809523384,0.09%,0.55%
1_2,first quarter,4,Model with 1_2 of layers (first quarter) quantized to 4 bits,18.25,9.0,8.0,38.9,41.25,25.33,51.91,20.69,4.11,0.15,57.74,5.56,8.64,22.93,83.17,19.19,64.0,7.6,23.44,23.67,26.43,26.664761904761907,Llama-2-7b-chat-hf,0.032380952380950845,0.18047619047618468,0.12%,0.68%
1_4,first quarter,4,Model with 1_4 of layers (first quarter) quantized to 4 bits,17.75,8.0,9.0,36.62,41.26,25.51,51.8,21.07,3.12,0.15,58.25,5.45,8.53,22.26,83.34,19.24,64.0,8.04,25.21,24.55,26.69,26.659047619047616,Llama-2-7b-chat-hf,0.005714285714290668,0.18619047619047535,0.02%,0.70%
3_4,first quarter,4,Model with 3_4 of layers (first quarter) quantized to 4 bits,17.75,8.5,7.5,38.93,41.75,25.06,52.1,20.45,4.06,0.15,58.27,5.69,8.28,23.6,82.76,18.83,64.0,7.97,23.18,24.19,26.68,26.65238095238095,Llama-2-7b-chat-hf,0.006666666666667709,0.19285714285714306,0.03%,0.72%
1_4,third quarter,4,Model with 1_4 of layers (third quarter) quantized to 4 bits,17.0,8.0,9.0,37.32,41.24,25.72,52.18,20.99,3.14,0.14,58.33,5.54,8.02,23.72,83.26,19.2,64.0,7.69,24.51,23.23,26.52,26.607142857142858,Llama-2-7b-chat-hf,0.045238095238090636,0.2380952380952337,0.17%,0.89%
3_4,second quarter,4,Model with 3_4 of layers (second quarter) quantized to 4 bits,18.75,7.0,9.5,39.06,40.95,25.69,51.89,20.51,3.45,0.14,57.95,5.2,8.18,23.34,82.88,18.83,64.0,7.68,22.86,23.43,26.43,26.558095238095234,Llama-2-7b-chat-hf,0.04904761904762367,0.28714285714285737,0.18%,1.08%
1_4,fourth quarter,2,Model with 1_4 of layers (fourth quarter) quantized to 2 bits,16.75,8.5,10.25,36.75,41.01,25.36,51.52,20.83,3.88,0.17,58.03,5.95,7.94,22.37,82.79,18.56,64.0,10.31,24.22,21.16,25.85,26.485714285714288,Llama-2-7b-chat-hf,0.07238095238094644,0.3595238095238038,0.27%,1.36%
1_4,first quarter,2,Model with 1_4 of layers (first quarter) quantized to 2 bits,14.5,4.25,7.08,29.15,39.85,23.84,47.79,19.79,5.14,1.37,51.55,7.21,7.74,26.82,82.39,18.2,64.0,20.16,19.43,22.92,26.52,25.699999999999996,Llama-2-7b-chat-hf,0.7857142857142918,1.1452380952380956,3.06%,4.46%
1_4,third quarter,2,Model with 1_4 of layers (third quarter) quantized to 2 bits,18.75,5.5,3.0,36.34,40.29,25.58,50.75,21.06,2.0,0.23,57.04,5.31,8.69,17.87,82.83,17.21,64.0,10.01,21.68,24.06,25.94,25.625714285714285,Llama-2-7b-chat-hf,0.07428571428571118,1.2195238095238068,0.29%,4.76%
1_2,second quarter,2,Model with 1_2 of layers (second quarter) quantized to 2 bits,18.0,5.75,5.25,35.82,40.84,25.94,48.77,20.79,2.07,0.2,55.3,6.72,7.53,17.86,82.38,16.97,64.0,11.52,20.49,22.02,25.16,25.399047619047614,Llama-2-7b-chat-hf,0.22666666666667012,1.446190476190477,0.89%,5.69%
1_4,second quarter,2,Model with 1_4 of layers (second quarter) quantized to 2 bits,18.25,6.0,4.17,34.25,38.09,24.58,49.53,20.45,3.31,0.17,54.39,5.34,9.48,23.78,83.03,18.3,64.5,5.25,19.47,26.41,24.07,25.37238095238095,Llama-2-7b-chat-hf,0.02666666666666373,1.4728571428571406,0.11%,5.80%
1_2,first quarter,2,Model with 1_2 of layers (first quarter) quantized to 2 bits,16.0,5.08,4.5,22.0,35.49,19.18,41.81,18.63,4.0,3.61,43.63,7.57,6.59,22.54,82.29,17.65,61.0,16.88,15.5,26.19,17.74,23.232380952380954,Llama-2-7b-chat-hf,2.139999999999997,3.6128571428571377,9.21%,15.55%
3_4,second quarter,2,Model with 3_4 of layers (second quarter) quantized to 2 bits,18.75,2.0,1.0,32.44,36.93,23.49,43.15,20.16,2.5,0.22,49.46,5.05,7.32,17.21,82.01,14.94,64.5,6.11,13.83,18.61,19.29,22.80809523809524,Llama-2-7b-chat-hf,0.4242857142857126,4.03714285714285,1.86%,17.70%
3_4,first quarter,2,Model with 3_4 of layers (first quarter) quantized to 2 bits,15.0,2.25,4.83,20.06,30.37,17.3,38.59,18.16,4.75,3.48,39.53,6.76,6.27,20.97,77.87,16.37,61.0,12.47,11.86,23.36,15.29,21.263809523809527,Llama-2-7b-chat-hf,1.5442857142857136,5.581428571428564,7.26%,26.25%
all,NA,2,Model with all layers quantized to 2 bits,15.0,2.88,2.52,16.55,28.89,15.63,35.78,18.0,5.0,4.64,37.72,7.4,7.01,11.82,77.51,12.93,62.5,12.26,11.53,19.65,14.0,19.96285714285714,Llama-2-7b-chat-hf,1.3009523809523884,6.882380952380952,6.52%,34.48%
